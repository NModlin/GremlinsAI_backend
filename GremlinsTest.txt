Comprehensive Testing Plan: GremlinsAI Backend
1. Introduction
This document outlines the comprehensive testing strategy for the GremlinsAI Backend. The primary goal is to validate the functionality, reliability, performance, and security of the application to ensure it is production-ready. This plan details the scope of testing, the methodologies to be used, the required tools, and provides examples of test cases.
Following this plan will establish a robust quality assurance process, enabling developers to add new features and refactor existing code with confidence.
2. Scope of Testing
In Scope
* API Endpoints: All public-facing API endpoints defined in the app/api/ directory.
* Agent and Crew Logic: The definition, configuration, and interaction of all CrewAI agents and crews.
* Agent Tools: The functionality of individual tools available to the agents.
* Service Layer: Business logic within the app/services/ directory, including interactions with the Qdrant database.
* Data Models: Pydantic models, including request/response validation and data structure integrity.
* Configuration: Validation of environment variable loading and application configuration.
Out of Scope
* Third-Party Services: The internal workings and performance of external services like the OpenAI API and the Qdrant database engine itself. Our testing will focus on the integration with these services.
* LLM Performance: We will not be testing the base capabilities (e.g., reasoning, language quality) of the underlying Large Language Models.
* Frontend Applications: Any client-side applications that consume this API are outside the scope of this plan.
3. Testing Objectives
* Ensure Functional Correctness: Verify that all features work as specified.
* Guarantee Reliability: Ensure the application is stable and handles errors gracefully.
* Validate Performance: Confirm the API can handle expected loads with acceptable response times.
* Strengthen Security: Identify and mitigate common API vulnerabilities.
* Automate Quality Assurance: Create a repeatable, automated test suite that can be integrated into a CI/CD pipeline.
4. Testing Strategy & Methodologies
We will employ a multi-layered testing strategy to cover the application from individual components to the system as a whole.
4.1. Unit Testing
Focuses on testing the smallest units of code (e.g., individual functions) in isolation.
* Targets: Helper functions, individual tool logic, Pydantic model validation.
* Methodology: All external dependencies (LLM calls, database connections) will be mocked. This ensures tests are fast, deterministic, and isolated.
* Tools: pytest, unittest.mock.
Example Test Case:
* Component: A tool function that formats a string.
* Test: Provide a sample string to the function and assert that the returned string has the expected format, without making any external API calls.
4.2. Integration Testing
Focuses on verifying the interaction between different components of the system.
* Targets: API endpoints interacting with service functions, services interacting with the database client.
* Methodology: Use a live test database (a separate, ephemeral Qdrant collection). External LLM API calls will still be mocked to control test outcomes and reduce cost/latency.
* Tools: pytest, FastAPI's TestClient.
Example Test Case:
* Flow: Testing the document upload endpoint (POST /v1/documents/).
* Test:
   1. Use TestClient to send a request with a file to the endpoint.
   2. Verify the API returns a 201 Created status code.
   3. Assert that the service layer function was called with the correct data.
   4. Connect to the test Qdrant database to confirm that a new vector/document has been created.
   5. Clean up the test database by deleting the created document.
4.3. End-to-End (E2E) Testing
Simulates a full user workflow from the API request to the final response, including interactions with live external services.
* Targets: Complex user scenarios, such as a full chat conversation that involves RAG (Retrieval-Augmented Generation).
* Methodology: These tests will use the TestClient to interact with the API. They will be the only tests that make actual, live calls to the Qdrant database and the LLM provider (OpenAI). Due to their slow speed and potential cost, they should be used sparingly for critical workflows.
* Tools: pytest, TestClient.
Example Test Case:
* Flow: A user asks a question about a previously uploaded document.
* Test:
   1. Setup: Use the API to upload a specific test document.
   2. Execution: Send a chat message to the /v1/chat/ endpoint with a question whose answer is contained only in the test document.
   3. Validation:
      * Assert that the response status code is 200 OK.
      * Assert that the response content contains the key information from the document. (This assertion needs to be flexible due to the non-deterministic nature of LLMs).
4.4. Performance Testing
Measures the application's responsiveness, stability, and resource consumption under load.
* Targets: High-traffic endpoints like /v1/chat/ and /v1/documents/search/.
* Methodology: Simulate concurrent users making requests to the API and measure key metrics.
* Tools: Locust.
Example Test Case:
* Scenario: Simulate 50 concurrent users sending chat messages.
* Metrics to Measure:
   * Average, median, and 95th percentile response times.
   * Requests per second (RPS).
   * Error rate (%).
   * Server CPU and memory usage.
4.5. Security Testing
Focuses on identifying and patching security vulnerabilities.
* Targets: Authentication, authorization, input validation, and data handling.
* Methodology: A combination of automated scanning and manual checks based on the OWASP API Security Top 10.
* Tools: Manual review, security scanning tools (e.g., OWASP ZAP).
Example Test Case:
* Vulnerability: Injection.
* Test: Send a request to an endpoint with a payload containing malicious input (e.g., SQL-like syntax, script tags) and verify that the application handles it gracefully (e.g., returns a 422 Unprocessable Entity error) instead of crashing or executing the input.
4.6. AI Agent Evaluation
A specialized form of testing to assess the quality and consistency of the AI agents' output.
* Targets: The reasoning and output quality of the CrewAI crews.
* Methodology: Use the crewai test CLI tool or build a custom evaluation suite. Create a "golden dataset" of prompts and their ideal, expected outcomes. The test runs the prompts and compares the agent's output to the ideal response. An LLM-as-judge pattern can be used to score the quality of the generated response.
* Tools: crewai test CLI, custom pytest scripts.
Example Test Case:
* Scenario: Evaluate the "researcher" agent's ability to summarize text.
* Test:
   1. Provide the agent with a standard article.
   2. Ask it to generate a one-paragraph summary.
   3. Compare the generated summary against a pre-written "golden" summary. The test passes if the semantic similarity score (calculated using another model or library) is above a certain threshold (e.g., 90%).
5. Recommended Tools & Frameworks
Category
	Tool
	Purpose
	Testing Framework
	pytest
	The core framework for writing and running all tests.
	Async Testing
	pytest-asyncio
	Enables pytest to run asynchronous test functions.
	HTTP Client
	TestClient (from FastAPI)
	For making requests to the API in tests without a live server.
	Mocking
	unittest.mock
	To isolate components by mocking dependencies.
	Code Coverage
	pytest-cov
	To measure how much of the codebase is covered by tests.
	Performance Testing
	Locust
	For load testing the API with simulated users.
	Linting/Formatting
	Ruff / Black
	To maintain code quality and consistency.
	6. Test Environment Setup
1. Configuration: A separate .env.test file should be created for the testing environment. It will point to a test database and use mock API keys where applicable.
2. Database: A dedicated Qdrant instance (or a separate collection within an existing instance) should be used exclusively for running tests. This database should be programmatically set up and torn down before and after the test suite runs.
3. CI/CD Integration: The test suite (specifically unit and integration tests) should be configured to run automatically on every pull request to the main branch using a platform like GitHub Actions.