#!/usr/bin/env python3
"""
Local LLM Setup Script for GremlinsAI

This script helps you set up and configure local LLM providers for GremlinsAI.
It will guide you through the installation and configuration process.
"""

import os
import sys
import subprocess
import json
import requests
from pathlib import Path
from typing import Dict, Any, Optional

def print_header(title: str):
    """Print a formatted header."""
    print("\n" + "="*60)
    print(f" {title}")
    print("="*60)

def print_step(step: str):
    """Print a formatted step."""
    print(f"\n🔧 {step}")

def print_success(message: str):
    """Print a success message."""
    print(f"✅ {message}")

def print_warning(message: str):
    """Print a warning message."""
    print(f"⚠️  {message}")

def print_error(message: str):
    """Print an error message."""
    print(f"❌ {message}")

def check_ollama_installed() -> bool:
    """Check if Ollama is installed."""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False

def check_ollama_running() -> bool:
    """Check if Ollama service is running."""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        return response.status_code == 200
    except:
        return False

def get_ollama_models() -> list:
    """Get list of installed Ollama models."""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
    except:
        pass
    return []

def install_python_dependencies():
    """Install required Python dependencies."""
    print_step("Installing Python dependencies for local LLMs...")
    
    dependencies = [
        'langchain-ollama',
        'langchain-huggingface',
        'transformers',
        'accelerate'
    ]
    
    for dep in dependencies:
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', dep], 
                         check=True, capture_output=True)
            print_success(f"Installed {dep}")
        except subprocess.CalledProcessError as e:
            print_error(f"Failed to install {dep}: {e}")

def setup_environment_file():
    """Set up the .env file with local LLM configuration."""
    print_step("Setting up environment configuration...")
    
    env_content = """# GremlinsAI Local LLM Configuration
# Generated by setup_local_llm.py

# -- Core Application Settings --
LOG_LEVEL="INFO"
DATABASE_URL="sqlite:///./data/gremlinsai.db"

# -- Local LLM Configuration --
# Ollama Configuration (recommended for local LLMs)
OLLAMA_BASE_URL="http://localhost:11434"
OLLAMA_MODEL="llama3.2:3b"

# Hugging Face Configuration (alternative local option)
USE_HUGGINGFACE="false"
HF_MODEL="microsoft/DialoGPT-medium"

# General LLM Settings
LLM_TEMPERATURE="0.1"
LLM_MAX_TOKENS="2048"

# -- Other Services --
QDRANT_HOST="localhost"
QDRANT_PORT="6333"
REDIS_URL="redis://localhost:6379"
EMBEDDING_MODEL="all-MiniLM-L6-v2"
"""
    
    with open('.env', 'w') as f:
        f.write(env_content)
    
    print_success("Created .env file with local LLM configuration")

def test_llm_configuration():
    """Test the LLM configuration."""
    print_step("Testing LLM configuration...")
    
    try:
        # Import and test the LLM configuration
        sys.path.append('.')
        from app.core.llm_config import get_llm_info, get_llm
        
        info = get_llm_info()
        print(f"📊 LLM Provider: {info['provider']}")
        print(f"📊 Model: {info['model_name']}")
        print(f"📊 Available: {info['available']}")
        
        if info['available']:
            print_success("LLM configuration is working!")
        else:
            print_warning("LLM is using mock provider - consider setting up Ollama")
            
    except Exception as e:
        print_error(f"Failed to test LLM configuration: {e}")

def main():
    """Main setup function."""
    print_header("GremlinsAI Local LLM Setup")
    
    print("This script will help you set up local LLM providers for GremlinsAI.")
    print("You can run GremlinsAI completely offline without API keys!")
    
    # Check current directory
    if not os.path.exists('app/main.py'):
        print_error("Please run this script from the GremlinsAI backend root directory")
        sys.exit(1)
    
    # Install Python dependencies
    install_python_dependencies()
    
    # Set up environment file
    setup_environment_file()
    
    # Check Ollama installation
    print_step("Checking Ollama installation...")
    
    if check_ollama_installed():
        print_success("Ollama is installed")
        
        if check_ollama_running():
            print_success("Ollama service is running")
            
            models = get_ollama_models()
            if models:
                print_success(f"Available models: {', '.join(models)}")
            else:
                print_warning("No models installed. Run: ollama pull llama3.2:3b")
        else:
            print_warning("Ollama service is not running. Start it with: ollama serve")
    else:
        print_warning("Ollama is not installed")
        print("📖 To install Ollama:")
        print("   Windows: Download from https://ollama.ai/download")
        print("   macOS: brew install ollama")
        print("   Linux: curl -fsSL https://ollama.ai/install.sh | sh")
    
    # Test configuration
    test_llm_configuration()
    
    print_header("Setup Complete!")
    
    print("🎯 Next Steps:")
    print("1. If you haven't installed Ollama, install it from https://ollama.ai")
    print("2. Start Ollama service: ollama serve")
    print("3. Download a model: ollama pull llama3.2:3b")
    print("4. Start GremlinsAI: uvicorn app.main:app --host 127.0.0.1 --port 8000")
    print("5. Test the API at: http://localhost:8000/docs")
    
    print("\n📚 For more information, see: docs/LOCAL_LLM_SETUP.md")

if __name__ == "__main__":
    main()
